{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ce094c",
   "metadata": {},
   "source": [
    "# Creación de subconjuntos\n",
    "\n",
    "Descargamos primero los [metadatos CSV](https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_Metadata.csv) del conjunto ISIC 2019 para seleccionar 2 subconjuntos independientes:\n",
    "- Conjunto 1: 100 imágenes de entrenamiento + 10 de test\n",
    "- Conjunto 2: 100 imágenes de entrenamiento + 10 de test\n",
    "\n",
    "Luego descargamos el [archivo ZIP completo](https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_Input.zip) pero *solo extraemos las 220 imágenes seleccionadas, evitando problemas de espacio en el GitHub Codespace de 10GB (~220 MB vs 9+ GB completo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f661e4e",
   "metadata": {},
   "source": [
    "## 1. Descarga del CSV y selección de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b98956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imágenes en el dataset: 25331\n",
      "Columnas: ['image', 'age_approx', 'anatom_site_general', 'lesion_id', 'sex']\n",
      "\n",
      "Primeras 5 filas:\n",
      "          image  age_approx anatom_site_general lesion_id     sex\n",
      "0  ISIC_0000000        55.0      anterior torso       NaN  female\n",
      "1  ISIC_0000001        30.0      anterior torso       NaN  female\n",
      "2  ISIC_0000002        60.0     upper extremity       NaN  female\n",
      "3  ISIC_0000003        30.0     upper extremity       NaN    male\n",
      "4  ISIC_0000004        80.0     posterior torso       NaN    male\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "csv_url = \"https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_Metadata.csv\"\n",
    "dest_dir = Path(\"conjunto_de_datos\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = dest_dir / \"ISIC_2019_Training_Metadata.csv\"\n",
    "\n",
    "with requests.get(csv_url, timeout=60) as r:\n",
    "    r.raise_for_status()\n",
    "    csv_path.write_bytes(r.content)\n",
    "\n",
    "# Carga el CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Total de imágenes en el dataset: {len(df)}\")\n",
    "print(f\"Columnas: {list(df.columns)}\\n\")\n",
    "\n",
    "# Muestra las primeras filas (Referencia a Ingeniería de Sistemas Intensivos en Datos jeje)\n",
    "print(\"Primeras 5 filas:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528a44f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles: ['image', 'age_approx', 'anatom_site_general', 'lesion_id', 'sex']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selección de 2 conjuntos (cada uno con 100 train + 10 test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Columnas disponibles: {list(df.columns)}\\n\")\n",
    "\n",
    "# Primero separamos 220 imágenes del dataset completo\n",
    "sample_size = 220\n",
    "if len(df) < sample_size:\n",
    "    df_subset = df\n",
    "else:\n",
    "    df_subset = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Primera división: 110 para conjunto 1, 110 para conjunto 2\n",
    "set1_df, set2_df = train_test_split(\n",
    "    df_subset,\n",
    "    train_size=110,\n",
    "    test_size=110,\n",
    "    stratify=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train1_df, test1_df = train_test_split(set1_df, train_size=100, test_size=10, random_state=42)\n",
    "train2_df, test2_df = train_test_split(set2_df, train_size=100, test_size=10, random_state=43)\n",
    "\n",
    "# Guarda los 4 subconjuntos\n",
    "train1_df.to_csv(dest_dir / \"train1_subset.csv\", index=False)\n",
    "test1_df.to_csv(dest_dir / \"test1_subset.csv\", index=False)\n",
    "train2_df.to_csv(dest_dir / \"train2_subset.csv\", index=False)\n",
    "test2_df.to_csv(dest_dir / \"test2_subset.csv\", index=False)\n",
    "\n",
    "\n",
    "# Verifica que NO hay solapamiento entre ningún par de conjuntos\n",
    "image_col = df.columns[0]\n",
    "train1_imgs = set(train1_df[image_col])\n",
    "test1_imgs = set(test1_df[image_col])\n",
    "train2_imgs = set(train2_df[image_col])\n",
    "test2_imgs = set(test2_df[image_col])\n",
    "\n",
    "all_sets = [\n",
    "    (\"train1\", train1_imgs),\n",
    "    (\"test1\", test1_imgs),\n",
    "    (\"train2\", train2_imgs),\n",
    "    (\"test2\", test2_imgs)\n",
    "]\n",
    "\n",
    "has_overlap = False\n",
    "for i, (name1, set1) in enumerate(all_sets):\n",
    "    for name2, set2 in all_sets[i+1:]:\n",
    "        overlap = set1 & set2\n",
    "        if overlap:\n",
    "            print(f\"ERROR: {len(overlap)} imágenes repetidas entre {name1} y {name2}\")\n",
    "            has_overlap = True\n",
    "\n",
    "# Lista completa de imágenes a extraer\n",
    "selected_images = train1_imgs | test1_imgs | train2_imgs | test2_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce66d953",
   "metadata": {},
   "source": [
    "## 2. Descarga selectiva del ZIP (solo 220 imágenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d161d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando el archivo ZIP completo...\n",
      "(Esto puede tardar varios minutos, ~9-10 GB)\n",
      "\n",
      "Descargado: 9.10 GB / 9.10 GB (100.0%)\n",
      "\n",
      "✓ Descarga completa. Extrayendo solo las 220 imágenes seleccionadas...\n",
      "\n",
      "Extraídas: 220/220 imágenes\r"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "zip_url = \"https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_Input.zip\"\n",
    "\n",
    "print(\"Descargando el archivo ZIP completo...\")\n",
    "print(\"(Esto puede tardar varios minutos, ~9-10 GB)\\n\")\n",
    "\n",
    "# Descarga con streaming a archivo temporal\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_file:\n",
    "    tmp_path = Path(tmp_file.name)\n",
    "    \n",
    "    with requests.get(zip_url, stream=True, timeout=600) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        downloaded = 0\n",
    "        \n",
    "        for chunk in r.iter_content(chunk_size=1024*1024):  # 1 MB\n",
    "            if chunk:\n",
    "                tmp_file.write(chunk)\n",
    "                downloaded += len(chunk)\n",
    "                if total_size > 0:\n",
    "                    pct = (downloaded / total_size) * 100\n",
    "                    print(f\"Descargado: {downloaded / (1024**3):.2f} GB / {total_size / (1024**3):.2f} GB ({pct:.1f}%)\", end='\\r')\n",
    "    \n",
    "    print(\"\\n\\n✓ Descarga completa. Extrayendo solo las 220 imágenes seleccionadas...\\n\")\n",
    "    \n",
    "    # Extrae SOLO las imágenes del subset\n",
    "    extracted_count = 0\n",
    "    not_found = []\n",
    "    \n",
    "    with zipfile.ZipFile(tmp_path, 'r') as z:\n",
    "        all_files = z.namelist()\n",
    "        \n",
    "        for img_name in selected_images:\n",
    "            # Busca el archivo en el zip (puede estar en una subcarpeta)\n",
    "            matching = [f for f in all_files if img_name in f and f.endswith('.jpg')]\n",
    "            \n",
    "            if matching:\n",
    "                for file_path in matching:\n",
    "                    z.extract(file_path, dest_dir)\n",
    "                    extracted_count += 1\n",
    "                    if extracted_count % 20 == 0:\n",
    "                        print(f\"Extraídas: {extracted_count}/{len(selected_images)} imágenes\", end='\\r')\n",
    "            else:\n",
    "                not_found.append(img_name)\n",
    "    \n",
    "    \n",
    "    if not_found:\n",
    "        print(f\"⚠ {len(not_found)} imágenes no encontradas en el ZIP:\")\n",
    "        print(not_found[:5], \"...\" if len(not_found) > 5 else \"\")\n",
    "\n",
    "# Limpia el archivo temporal\n",
    "tmp_path.unlink(missing_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
